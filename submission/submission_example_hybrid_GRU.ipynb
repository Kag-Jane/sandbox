{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example notebook submission\n",
    "This notebook is a successful submission. It was submitted and scored without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import polars as pls\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is already defined as `model`\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model uploaded via kaggle UI\n",
    "#path_to_model_lgb = \"/kaggle/input/test_lgbm_null_to_0/other/default/1/jane_lgbm.txt\"\n",
    "path_to_model_lgb = \"../mono_model/model_init/jane_lgbm_baseline.txt\"\n",
    "#path_to_model_nn = \"/kaggle/input/test_lgbm_null_to_0/other/default/1/jane_nn.ckpt\"\n",
    "path_to_model_nn = \"../hybrid_model/model_init/jane_gru_hidden_64_layer_2_rmse.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRURegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 2, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        # Define GRU layer\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define a fully connected layer to map GRU outputs to a single value\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        self.criterion = nn.MSELoss()\n",
    "        #self.criterion = r2_score\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU\n",
    "        #_, hidden = self.gru(x)  # hidden is the last hidden state\n",
    "        outputs, _ = self.gru(x)\n",
    "        \n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        #output = self.fc(hidden[-1])\n",
    "        output = self.fc(outputs)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y.squeeze())\n",
    "        self.training_step_outputs.append(loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_average = torch.tensor(self.training_step_outputs).mean()\n",
    "        self.log(\"training_epoch_average\", epoch_average)\n",
    "        self.training_step_outputs.clear()  # free memory\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y.squeeze())\n",
    "        self.validation_step_outputs.append(loss.item())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = torch.tensor(self.validation_step_outputs).mean()\n",
    "        self.log(\"avg_val_loss\", avg_val_loss)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.criterion(y_hat, y.squeeze())\n",
    "        self.test_step_outputs.append(loss.item())\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return {\"test_loss\": loss}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        epoch_average = torch.tensor(self.test_step_outputs).mean()\n",
    "        self.log(\"test_epoch_average\", epoch_average)\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model to make predictions\n",
    "model_lgb = lgb.Booster(model_file=path_to_model_lgb)\n",
    "model_nn = GRURegressor.load_from_checkpoint(path_to_model_nn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRURegressor(\n",
       "  (gru): GRU(81, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loader for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_list = [\"time_id\", \"symbol_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, df: pls.DataFrame):\n",
    "        df = df.fill_null(0)\n",
    "        self.features = torch.tensor(df.select([col for col in df.columns if col in train_feature_list]).to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with testing dataset\n",
    "def test_dataloader(df: pls.DataFrame, batch_size: int = 10000):\n",
    "    dataset = TimeseriesDataset(df)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=15, multiprocessing_context='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pls.DataFrame, lags: pls.DataFrame | None) -> pls.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    \n",
    "    # Inference with LGBM\n",
    "    y_pred_lgb = model_lgb.predict(test.select([col for col in train_feature_list]))\n",
    "\n",
    "    # Inference with NN\n",
    "    nn_predictions = []\n",
    "\n",
    "    # Use dataloader to load all batches\n",
    "    data_loader = test_dataloader(test, batch_size=10000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_pred = model_nn(batch.to(device)).squeeze()\n",
    "            nn_predictions.append(y_pred)\n",
    "\n",
    "    y_pred_nn = torch.cat(nn_predictions, dim=0).cpu().numpy()\n",
    "\n",
    "    # Combine the predictions from the two models\n",
    "    predictions = test.select(pls.col(\"row_id\"))\n",
    "    y_pred = (y_pred_lgb + y_pred_nn) / 2\n",
    "    predictions = predictions.with_columns(pls.Series(\"responder_6\", y_pred))\n",
    "\n",
    "    if isinstance(predictions, pls.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    # Confirm has as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/.pyenv/versions/3.11.10/envs/ml/lib/python3.11/site-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n",
      "/home/yang/.pyenv/versions/3.11.10/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "/home/yang/.pyenv/versions/3.11.10/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>responder_6</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0.017081</td></tr><tr><td>1</td><td>0.013513</td></tr><tr><td>2</td><td>0.013513</td></tr><tr><td>3</td><td>0.013513</td></tr><tr><td>4</td><td>0.013513</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>0.013513</td></tr><tr><td>35</td><td>0.013513</td></tr><tr><td>36</td><td>0.013513</td></tr><tr><td>37</td><td>0.013513</td></tr><tr><td>38</td><td>0.013513</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 2)\n",
       "┌────────┬─────────────┐\n",
       "│ row_id ┆ responder_6 │\n",
       "│ ---    ┆ ---         │\n",
       "│ i64    ┆ f64         │\n",
       "╞════════╪═════════════╡\n",
       "│ 0      ┆ 0.017081    │\n",
       "│ 1      ┆ 0.013513    │\n",
       "│ 2      ┆ 0.013513    │\n",
       "│ 3      ┆ 0.013513    │\n",
       "│ 4      ┆ 0.013513    │\n",
       "│ …      ┆ …           │\n",
       "│ 34     ┆ 0.013513    │\n",
       "│ 35     ┆ 0.013513    │\n",
       "│ 36     ┆ 0.013513    │\n",
       "│ 37     ┆ 0.013513    │\n",
       "│ 38     ┆ 0.013513    │\n",
       "└────────┴─────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the pipeline using the example test data\n",
    "data_path = \"/home/yang/kaggle/jane/data\"\n",
    "# load example test data\n",
    "test_data = pls.read_parquet(Path(data_path, \"test.parquet\", \"date_id=0\", \"part-0.parquet\"))\n",
    "\n",
    "predictions = predict(test_data, None)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
