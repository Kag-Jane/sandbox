{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4a958d-2c78-4ed2-a491-fcb047294ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "\n",
    "data_path = \"/Users/Sigrid/Desktop/JS\"\n",
    "feature_tags = pl.read_csv(Path(data_path, 'features.csv'))\n",
    "\n",
    "# for each training set, we take 20% of the data for validation\n",
    "frac_train = 0.8\n",
    "train_raw_data_num = [\"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "# a completely new dataset for testing\n",
    "test_raw_data_num = \"1\"\n",
    "test_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={test_raw_data_num}\", \"part-0.parquet\"))\n",
    "\n",
    "def sample_weighted_r2(y_pred, y_truth, weight):\n",
    "    \"\"\"\n",
    "    Zero-mean R-squared metrics.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Array of predicted values.\n",
    "        y_truth: Array of true values.\n",
    "        weight: Array of sample weights.\n",
    "\n",
    "    Returns:\n",
    "        1-corr: Zero-mean R-squared.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure weights are valid\n",
    "    weight = weight if weight is not None else np.ones_like(y_pred)\n",
    "    \n",
    "    corr = np.sum((weight * (y_truth - y_pred) ** 2)) / np.sum(weight * y_truth ** 2)\n",
    "    \n",
    "    return 1 - corr \n",
    "\n",
    "# Set parameters for LightGBM, rmse\n",
    "params_rmse = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43b6a41-207e-4aeb-81b9-d3e14f9198a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data (GB): 1.8544514616951346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sigrid/Desktop/JS/trial/.venv/lib/python3.11/site-packages/lightgbm/basic.py:357: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning(\"Converting column-vector to 1d array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.339748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19610\n",
      "[LightGBM] [Info] Number of data points in the train set: 4278560, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score -0.006785\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\ttrain's rmse: 0.845973\tval's rmse: 0.846929\n",
      "Size of training data (GB): 2.149294967763126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sigrid/Desktop/JS/trial/.venv/lib/python3.11/site-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.614510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19617\n",
      "[LightGBM] [Info] Number of data points in the train set: 4963129, number of used features: 81\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\ttrain's rmse: 0.818328\tval's rmse: 1.06698\n",
      "Size of training data (GB): 2.1955207837745547\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.709914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19615\n",
      "[LightGBM] [Info] Number of data points in the train set: 5068448, number of used features: 81\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\ttrain's rmse: 0.78408\tval's rmse: 0.820134\n",
      "Size of training data (GB): 2.126415732316673\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.798923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19616\n",
      "[LightGBM] [Info] Number of data points in the train set: 4912019, number of used features: 81\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\ttrain's rmse: 0.836812\tval's rmse: 0.634189\n",
      "Size of training data (GB): 2.173490112647414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.712669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19618\n",
      "[LightGBM] [Info] Number of data points in the train set: 5019660, number of used features: 81\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttrain's rmse: 0.799332\tval's rmse: 0.742786\n",
      "-0.0030285357781298927\n",
      "[ 5.75582901e-02  8.14726632e-02  1.99136434e-01 ... -7.02847666e-03\n",
      "  9.90253066e-03  1.72654243e-04]\n"
     ]
    }
   ],
   "source": [
    "## 0.025: 0.0038\n",
    "## 0.01: 0.0042\n",
    "\n",
    "train_feature_list = [\"time_id\", \"symbol_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]\n",
    "\n",
    "model_name = 'jane_lgbm_baseline.txt'\n",
    "\n",
    "# initialize the model\n",
    "model = None\n",
    "\n",
    "evals_result = {}\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in train_raw_data_num:\n",
    "    training_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={i}\", \"part-0.parquet\"))\n",
    "    training_data = training_data.with_columns((np.exp(-0.01*(training_data['date_id'].max() - training_data['date_id']))).alias('time_decay'))  ## adding time-decay\n",
    "    training_data = training_data.with_columns((training_data['weight']*training_data['time_decay']).alias('weight_new'))\n",
    "    training_data = training_data.rename({'weight': 'weight_old', 'weight_new': 'weight'})\n",
    "    print(\"Size of training data (GB):\", training_data.estimated_size(\"gb\"))\n",
    "\n",
    "    #################################################################################################\n",
    "    ####################   Preprocess the training data and select features   #######################\n",
    "    #################################################################################################\n",
    "    training_data = training_data.fill_null(0)\n",
    "    training_data_subset = training_data.select([col for col in training_data.columns if col in train_feature_list])\n",
    "    #################################################################################################\n",
    "    label = training_data.select(pl.col(\"responder_6\"))\n",
    "    weight = training_data.select(pl.col(\"weight\"))\n",
    "    del training_data  # save memory\n",
    "    # Split the data into training and validation sets\n",
    "    split_index = int(frac_train * training_data_subset.shape[0])\n",
    "    training_data_loader = lgb.Dataset(training_data_subset[:split_index], label=label[:split_index].to_numpy(),\n",
    "                                       weight=weight[:split_index].to_numpy())\n",
    "    \n",
    "    validate_data_loader = lgb.Dataset(training_data_subset[split_index:], label=label[split_index:].to_numpy(),\n",
    "                                       reference=training_data_loader, weight=weight[split_index:].to_numpy())\n",
    "    \n",
    "    # Train the model\n",
    "    model = lgb.train(params_rmse, training_data_loader, init_model=model, num_boost_round=10,\n",
    "                      valid_sets=[training_data_loader, validate_data_loader],\n",
    "                      valid_names=['train', 'val'],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=5), lgb.record_evaluation(evals_result)]\n",
    "    )\n",
    "model.save_model('lgbm_' + model_name + '.txt')\n",
    "eval_df = pl.DataFrame({'train': evals_result['train']['rmse'], 'val': evals_result['val']['rmse']})\n",
    "eval_df.write_csv('eval_'+ model_name + '.csv')\n",
    "\n",
    "test_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={test_raw_data_num}\", \"part-0.parquet\"))\n",
    "test_data_subset = test_data.select([col for col in test_data.columns if col in train_feature_list])\n",
    "test_data.estimated_size(\"gb\")\n",
    "test_data = test_data.sort(['symbol_id', 'date_id', 'time_id'])\n",
    "\n",
    "# load saved model to make predictions\n",
    "model = lgb.Booster(model_file='lgbm_' + model_name + '.txt')\n",
    "y_pred = model.predict(test_data_subset)\n",
    "score = sample_weighted_r2(y_pred, test_data.select(pl.col(\"responder_6\")).to_numpy()[:,0],test_data.select(pl.col(\"weight\")).to_numpy()[:,0])\n",
    "print(score)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
