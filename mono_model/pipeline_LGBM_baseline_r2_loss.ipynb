{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jane Street Real-Time Market Data Forecasting baseline with LightGBM\n",
    "\n",
    "Link to the competition: https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important information\n",
    "\n",
    "- Lags: Values of responder_{0...8} lagged by one date_id. The evaluation API serves the entirety of the lagged responders for a date_id on that date_id's first time_id. In other words, all of the previous date's responders will be served at the first time step of the succeeding date.\n",
    "\n",
    "- The symbol_id column contains encrypted identifiers. Each symbol_id is not guaranteed to appear in all time_id and date_id combinations. Additionally, new symbol_id values may appear in future test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the zero-mean R-squared function as the loss and customize the evaluation metric.\n",
    "\n",
    "The zero-mean R-squared function is:\n",
    "\n",
    "$$ 1 - \\frac{\\sum_{i=1}^n w_i (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n w_i y_i^2} $$\n",
    "\n",
    "So the loss function is:\n",
    "\n",
    "$$ \\text{Loss} = \\sum_{i=1}^n w_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "To incorporate the zero-mean R-squared into the training loss in LightGBM, we need to calculate the gradient and hessian, which are:\n",
    "\n",
    "$$ \\frac{\\partial \\text{Loss}}{\\partial \\hat{y}_i} = -2 w_i (y_i - \\hat{y}_i) $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial^2 \\text{Loss}}{\\partial \\hat{y}_i^2} = 2 w_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we don't use lags at the moment. For more information about using lags data, check this [notebook](https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/yang/kaggle/jane/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each training set, we take 20% of the data for validation\n",
    "frac_train = 0.8\n",
    "train_raw_data_num = [\"0\", \"1\", \"2\", \"4\", \"5\", \"6\", \"8\", \"9\"]\n",
    "# a completely new dataset for testing\n",
    "test_raw_data_num = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_list = [\"time_id\", \"symbol_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_zero_mean_r2_objective(pred, train):\n",
    "    \"\"\"\n",
    "    Custom zero-mean R-squared objective for LightGBM.\n",
    "\n",
    "    Args:\n",
    "        y_true: Array of true values.\n",
    "        y_pred: Array of predicted values.\n",
    "        weight: Array of sample weights.\n",
    "\n",
    "    Returns:\n",
    "        grad: Gradient.\n",
    "        hess: Hessian.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure weights are valid\n",
    "    weight = train.get_weight() if train.get_weight() is not None else np.ones_like(pred)\n",
    "    \n",
    "    # Gradient (negative derivative of the loss)\n",
    "    grad = -2 * (train.get_label() - pred) / (train.get_label() ** 2)\n",
    "    \n",
    "    # Hessian (second derivative of the loss)\n",
    "    hess = 2 * weight / (train.get_label() ** 2)\n",
    "    \n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weighted_zero_mean_r2(y_pred, y_truth, weight):\n",
    "    \"\"\"\n",
    "    Zero-mean R-squared metrics.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Array of predicted values.\n",
    "        y_truth: Array of true values.\n",
    "        weight: Array of sample weights.\n",
    "\n",
    "    Returns:\n",
    "        1-corr: Zero-mean R-squared.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure weights are valid\n",
    "    weight = weight if weight is not None else np.ones_like(y_pred)\n",
    "    \n",
    "    corr = np.sum((weight * (y_truth - y_pred) ** 2)) / np.sum(weight * y_truth ** 2)\n",
    "    \n",
    "    return 1 - corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": sample_zero_mean_r2_objective,  # Disable default objectives\n",
    "    \"metric\": \"None\",     # Disable default metrics\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data (GB): 0.6481935195624828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/.pyenv/versions/3.11.10/envs/ml/lib/python3.11/site-packages/lightgbm/basic.py:357: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning(\"Converting column-vector to 1d array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17282\n",
      "[LightGBM] [Info] Number of data points in the train set: 1555368, number of used features: 72\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 0.933896447531879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/.pyenv/versions/3.11.10/envs/ml/lib/python3.11/site-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18550\n",
      "[LightGBM] [Info] Number of data points in the train set: 2243397, number of used features: 77\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 1.009834710508585\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18581\n",
      "[LightGBM] [Info] Number of data points in the train set: 2429498, number of used features: 77\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 1.666749034076929\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.604943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19601\n",
      "[LightGBM] [Info] Number of data points in the train set: 4018361, number of used features: 81\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 1.7741344589740038\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.636657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19610\n",
      "[LightGBM] [Info] Number of data points in the train set: 4278560, number of used features: 81\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 2.056127244606614\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.667763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19617\n",
      "[LightGBM] [Info] Number of data points in the train set: 4963129, number of used features: 81\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 2.0342074520885944\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.676354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19616\n",
      "[LightGBM] [Info] Number of data points in the train set: 4912019, number of used features: 81\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Size of training data (GB): 2.079261187463999\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.650787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19618\n",
      "[LightGBM] [Info] Number of data points in the train set: 5019660, number of used features: 81\n",
      "[LightGBM] [Info] Using self-defined objective function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f6acbd53410>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = None\n",
    "\n",
    "evals_result = {}\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in train_raw_data_num:\n",
    "    training_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={i}\", \"part-0.parquet\"))\n",
    "    print(\"Size of training data (GB):\", training_data.estimated_size(\"gb\"))\n",
    "\n",
    "    #################################################################################################\n",
    "    ####################   Preprocess the training data and select features   #######################\n",
    "    #################################################################################################\n",
    "    training_data = training_data.fill_null(0)\n",
    "    training_data_subset = training_data.select([col for col in training_data.columns if col in train_feature_list])\n",
    "    #################################################################################################\n",
    "    label = training_data.select(pl.col(\"responder_6\"))\n",
    "    weight = training_data.select(pl.col(\"weight\"))\n",
    "    del training_data  # save memory\n",
    "    # Split the data into training and validation sets\n",
    "    split_index = int(frac_train * training_data_subset.shape[0])\n",
    "    training_data_loader = lgb.Dataset(training_data_subset[:split_index], label=label[:split_index].to_numpy(),\n",
    "                                       weight=weight[:split_index].to_numpy())\n",
    "    \n",
    "    validate_data_loader = lgb.Dataset(training_data_subset[split_index:], label=label[split_index:].to_numpy(),\n",
    "                                       reference=training_data_loader, weight=weight[split_index:].to_numpy())\n",
    "    \n",
    "    # Train the model\n",
    "    model = lgb.train(params, training_data_loader, init_model=model, num_boost_round=100, force_col_wise=True\n",
    "                      #valid_sets=[training_data_loader, validate_data_loader],\n",
    "                      #valid_names=['train', 'val'],\n",
    "                      #feval=sample_weighted_zero_mean_r2,\n",
    "                      #callbacks=[lgb.record_evaluation(evals_result)],\n",
    "                      #callbacks=[lgb.early_stopping(stopping_rounds=5), lgb.record_evaluation(evals_result)],\n",
    "    )\n",
    "\n",
    "    # Access validation loss\n",
    "    # training_loss.append(evals_result['train']['rmse'][-1])\n",
    "    # validation_loss.append(evals_result['val']['rmse'][-1])\n",
    "    # print(\"Training Losses per iteration:\", training_loss)\n",
    "    # print(\"Validation Losses per iteration:\", validation_loss)\n",
    "\n",
    "model.save_model('jane_lgbm_null_to_0_r2_loss.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1003760267049074"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={test_raw_data_num}\", \"part-0.parquet\"))\n",
    "test_data_subset = test_data.select([col for col in test_data.columns if col in train_feature_list])\n",
    "test_data.estimated_size(\"gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model to make predictions\n",
    "model = lgb.Booster(model_file='jane_lgbm_null_to_0_r2_loss.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/.pyenv/versions/3.11.10/envs/ml/lib/python3.11/site-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00014819, 0.00018369, 0.00014862, ..., 0.00031583, 0.00164334,\n",
       "       0.00051897])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_data_subset)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.868505102091248e-06)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = sample_weighted_zero_mean_r2(y_pred, test_data.select(pl.col(\"responder_6\")).to_numpy()[:,0],\n",
    "                                     test_data.select(pl.col(\"weight\")).to_numpy()[:,0])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags\n",
    "test_data = pl.read_parquet(Path(data_path, \"train.parquet\", f\"partition_id={test_raw_data_num}\", \"part-0.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
